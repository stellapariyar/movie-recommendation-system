{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies= pd.read_csv('movies_metadata.csv', low_memory=False)\n",
    "links= pd.read_csv('links.csv')\n",
    "credits=pd.read_csv('credits.csv')\n",
    "keywords=pd.read_csv('keywords.csv')\n",
    "ratings=pd.read_csv('ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "invalid_rows = movies[~movies['id'].apply(pd.to_numeric, errors='coerce').notnull()]\n",
    "print(invalid_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric_ids = movies[~movies['id'].str.isnumeric()]['id']\n",
    "\n",
    "print(non_numeric_ids)\n",
    "\n",
    "movies = movies[movies['id'].str.isnumeric()]\n",
    "\n",
    "movies['id'] = movies['id'].astype('int64')\n",
    "\n",
    "merged_df = pd.merge(movies, credits, on='id', how='left')\n",
    "\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = movies.copy()\n",
    "\n",
    "movies['id'] = movies['id'].astype(str)\n",
    "\n",
    "movies = movies[movies['id'].str.isnumeric()]\n",
    "\n",
    "movies['id'] = movies['id'].astype('int64')\n",
    "\n",
    "merged_df = pd.merge(movies, credits, on='id', how='left')\n",
    "\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(merged_df, keywords, on='id', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(links.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(merged_df, links, left_on='id', right_on='movie_id', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(merged_df, ratings, on='movie_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[['id', 'original_title', 'overview','genres', 'cast', 'crew']]\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[['id', 'original_title', 'overview','genres', 'cast', 'crew']].copy()\n",
    "merged_df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.iloc[0].genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def convert(obj):\n",
    "    L=[]\n",
    "    for i in ast.literal_eval(obj):\n",
    "        L.append(i['name'])\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['genres']=merged_df['genres'].apply(convert)\n",
    "merged_df['genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2(obj):\n",
    "    L=[]\n",
    "    counter=0\n",
    "    for i in ast.literal_eval(obj):\n",
    "        if counter!=3:\n",
    "         L.append(i['name'])\n",
    "         counter+1\n",
    "        else:\n",
    "           break\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['cast']=merged_df['cast'].apply(convert2)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "merged_df[\"crew\"] = merged_df[\"crew\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_director(crew_list):\n",
    "    if isinstance(crew_list, list):  # Check if it's a list\n",
    "        for member in crew_list:\n",
    "            if member.get(\"job\") == \"Director\": \n",
    "                return member.get(\"name\") \n",
    "    return None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"crew\"] = merged_df[\"crew\"].apply(get_director)\n",
    "merged_df['crew']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in [\"overview\",\"genres\", \"cast\", \"crew\"]:\n",
    "    merged_df[col] = merged_df[col].astype(str)# Convert list-type columns to strings\n",
    "\n",
    "merged_df = merged_df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['overview'] = merged_df['overview'].apply(lambda x: x.split() if isinstance(x, str) else x)\n",
    "\n",
    "\n",
    "merged_df['overview']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['tags'] = merged_df['genres'].astype(str) + \" \" + \\\n",
    "                    merged_df['cast'].astype(str) + \" \" + \\\n",
    "                    merged_df['crew'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=merged_df[['id','original_title','tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem (text):\n",
    "    y=[]\n",
    "    for i in text.split():\n",
    "     y.append(ps.stem (i))\n",
    "    return \" \".join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.loc[:,'tags']= new_df['tags'].apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_tags(tag):\n",
    "    if isinstance(tag, str):\n",
    "        # Extract only the words inside square brackets\n",
    "        matches = re.findall(r\"\\[([^\\]]+)\\]\", tag)\n",
    "        if matches:\n",
    "            # Flatten list and join as a single string\n",
    "            return \" \".join([item.replace(\"'\", \"\").strip() for sublist in matches for item in sublist.split(\",\")])\n",
    "    return str(tag)\n",
    "\n",
    "new_df.loc[:, 'tags'] = new_df['tags'].apply(clean_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['tags'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.loc[:, 'tags'] = new_df['tags'].apply(lambda x:x.lower())\n",
    "new_df['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_features=5000,stop_words='english')\n",
    "\n",
    "vector = cv.fit_transform(new_df['tags'].to_numpy())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = cv.fit_transform(new_df['tags'].to_numpy())  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vector.toarray()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "vector = cv.fit_transform(new_df['tags'])\n",
    "\n",
    "# Compute similarity matrix\n",
    "similarity = cosine_similarity(vector)\n",
    "\n",
    "print(similarity.shape)  # Verify similarity matrix size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(movies):\n",
    "    movie_index=new_df[new_df['original_title']==movies].index[0]\n",
    "    distances= similarity[movie_index]\n",
    "    movie_list=sorted(list(enumerate(distances)),reverse=True,key=lambda x:x[1])[1:6]\n",
    "\n",
    "    for i in movie_list:\n",
    "        print(new_df.iloc[i[0]].original_title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend('Toy Story')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(new_df,open('movies.pk1','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(similarity,open('similarity.pk1','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(similarity))  # <class 'numpy.ndarray'>\n",
    "print(similarity.shape)  # (num_movies, num_movies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "print(f\"Memory Usage: {psutil.virtual_memory().percent}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "movies = pickle.load(open('movies.pk1', 'rb'))\n",
    "\n",
    "print(\"Columns in movies dataset:\", movies.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "movies = pickle.load(open('movies.pk1', 'rb'))\n",
    "\n",
    "\n",
    "if 'tags' not in movies.columns:\n",
    "    raise KeyError(\" Error: 'tags' column not found in dataset!\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "vector = vectorizer.fit_transform(movies['tags'].fillna(''))\n",
    "\n",
    "pickle.dump(vector, open('vector.pk1', 'wb'))\n",
    "print(\" vector.pk1 generated successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "movies = pickle.load(open('movies.pk1', 'rb'))\n",
    "vector = pickle.load(open('vector.pk1', 'rb'))\n",
    "\n",
    "# If vector is sparse, keep it sparse\n",
    "if isinstance(vector, csr_matrix):\n",
    "    vector = vector.tocsr()\n",
    "\n",
    "#Use Nearest Neighbors for fast search\n",
    "nn_model = NearestNeighbors(n_neighbors=6, metric='cosine', algorithm='brute')\n",
    "nn_model.fit(vector)\n",
    "\n",
    "\n",
    "precomputed_recommendations = {}\n",
    "\n",
    "for i in range(len(movies)):\n",
    "    distances, indices = nn_model.kneighbors(vector[i], return_distance=True)\n",
    "    precomputed_recommendations[movies.iloc[i].original_title] = [\n",
    "        movies.iloc[j].original_title for j in indices[0][1:]  # Skip first (itself)\n",
    "    ]\n",
    "\n",
    "\n",
    "with open('precomputed_recommendations.pk1', 'wb') as f:\n",
    "    pickle.dump(precomputed_recommendations, f)\n",
    "\n",
    "print(\" Precomputed recommendations saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "\n",
    "# Match movie title to the closest name in dataset\n",
    "def get_best_match(movie_name):\n",
    "    matches = process.extractOne(movie_name, movies['original_title'].values)\n",
    "    return matches[0] if matches else None\n",
    "\n",
    "#Update Recommendation Function\n",
    "def recommend(movie):\n",
    "    best_match = get_best_match(movie)  # Find the closest matching movie\n",
    "    if not best_match:\n",
    "        return [\"No matching movie found.\"]\n",
    "\n",
    "    movie_index = movies[movies['original_title'] == best_match].index[0]\n",
    "    distances, indices = nn_model.kneighbors(vector[movie_index], return_distance=True)\n",
    "    \n",
    "    return [movies.iloc[j].original_title for j in indices[0][1:]]  # Skip first (itself)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
